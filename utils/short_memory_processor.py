# utils/short_memory_processor.py

import re
import json
import logging
from typing import List, Dict, Tuple, Optional
from datetime import datetime
try:
    import requests
except ImportError:
    # requestsãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯httpxã‚’ä½¿ç”¨
    import httpx as requests

logger = logging.getLogger(__name__)

class ShortMemoryProcessor:
    """çŸ­æœŸè¨˜æ†¶å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.glossary_cache = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã®è¾æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.stm_chunk = []  # åŒä¸€ãƒˆãƒ”ãƒƒã‚¯æŸã®ä¸€æ™‚è“„ç©
        self.stm_last_topic_repr = ""  # ç›´è¿‘ä»£è¡¨æ–‡
        self.load_glossary_cache()
    
    def load_glossary_cache(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã«è¾æ›¸ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ­ãƒ¼ãƒ‰"""
        try:
            # nekota-serverã®APIã‚’å‘¼ã³å‡ºã—
            api_url = "https://nekota-server-production.up.railway.app/api/memory"
            headers = {"Authorization": f"Bearer {self.get_jwt_token()}"}
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data and data.get("glossary"):
                    self.glossary_cache = data["glossary"]
            else:
                self.glossary_cache = {}
            logger.info(f"Loaded glossary cache for user {self.user_id}: {len(self.glossary_cache)} terms")
        except Exception as e:
            logger.error(f"Error loading glossary cache: {e}")
            self.glossary_cache = {}
    
    def get_jwt_token(self):
        """JWTãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å–å¾—
        return "dummy_token"
    
    def extract_candidate_terms(self, text: str) -> List[str]:
        """å€™è£œèªã‚’æŠ½å‡ºï¼ˆå›ºæœ‰åè©ãƒ»ã‚«ã‚¿ã‚«ãƒŠèªãƒ»è¨˜å·æ··ã˜ã‚Šã®çèªãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼å£ç™–ï¼‰"""
        # æ­£è¦è¡¨ç¾ã§å€™è£œèªã‚’æŠ½å‡º
        pattern = r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥A-Za-z0-9]{2,})'
        matches = re.findall(pattern, text)
        
        # æ­£è¦åŒ–ã‚­ãƒ¼ã‚’ä½œæˆï¼ˆã²ã‚‰ãŒãªåŒ–/å°æ–‡å­—åŒ–/ç©ºç™½é™¤å»ï¼‰
        normalized_terms = []
        for match in matches:
            normalized = self.normalize_term(match)
            if normalized and len(normalized) >= 2:
                normalized_terms.append(normalized)
        
        return list(set(normalized_terms))  # é‡è¤‡é™¤å»
    
    def normalize_term(self, term: str) -> str:
        """èªå¥ã‚’æ­£è¦åŒ–"""
        # ã²ã‚‰ãŒãªåŒ–ã€å°æ–‡å­—åŒ–ã€ç©ºç™½é™¤å»
        term = term.lower().strip()
        # ç°¡å˜ãªæ­£è¦åŒ–ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã‚ˆã‚Šè©³ç´°ãªå‡¦ç†ãŒå¿…è¦ï¼‰
        return term
    
    def check_glossary_reference(self, terms: List[str]) -> Dict[str, str]:
        """è¾æ›¸å‚ç…§ï¼ˆãƒ¡ãƒ¢ãƒªå‚ç…§ã®ã¿ã€DB I/Oãªã—ï¼‰"""
        found_meanings = {}
        for term in terms:
            if term in self.glossary_cache:
                found_meanings[term] = self.glossary_cache[term]
        return found_meanings
    
    def detect_topic_boundary(self, current_text: str) -> bool:
        """è©±é¡Œå¢ƒç•Œåˆ¤å®šï¼ˆAIä¸ä½¿ç”¨ï¼‰"""
        # åˆå›ç™ºè©±ã®å ´åˆã¯å¿…ãšå¢ƒç•Œã¨ã™ã‚‹
        if not self.stm_last_topic_repr:
            logger.info(f"Topic boundary detected: first utterance")
            return True
        
        # ãƒã‚¤ã‚ºåˆæµãƒã‚§ãƒƒã‚¯ï¼ˆçŸ­æ–‡ã¯å¢ƒç•Œã«ã—ãªã„ï¼‰
        if current_text.strip() in ["ã¯ã„", "äº†è§£", "ã†ã‚“", "ãã†", "ãªã‚‹ã»ã©"]:
            logger.info(f"Topic boundary not detected: noise response")
            return False
        
        # é¡ä¼¼åº¦é–¾å€¤ãƒã‚§ãƒƒã‚¯
        similarity = self.calculate_jaccard_similarity(
            self.stm_last_topic_repr, current_text
        )
        if similarity < 0.5:  # é–¾å€¤ã‚’ä¸‹ã’ã¦å¢ƒç•Œã‚’æ¤œå‡ºã—ã‚„ã™ãã™ã‚‹
            logger.info(f"Topic boundary detected by similarity: {similarity}")
            return True
        
        # æ–°è¦å›ºæœ‰èªã®å‡ºç¾ãƒã‚§ãƒƒã‚¯
        current_terms = self.extract_candidate_terms(current_text)
        last_terms = self.extract_candidate_terms(self.stm_last_topic_repr)
        new_terms = set(current_terms) - set(last_terms)
        if len(new_terms) >= 1:  # é–¾å€¤ã‚’ä¸‹ã’ã‚‹
            logger.info(f"Topic boundary detected by new terms: {new_terms}")
            return True
        
        # ä¼šè©±ã®é•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆstm_chunkãŒä¸€å®šæ•°ã«é”ã—ãŸã‚‰å¢ƒç•Œï¼‰
        if len(self.stm_chunk) >= 3:
            logger.info(f"Topic boundary detected: chunk length reached {len(self.stm_chunk)}")
            return True
        
        logger.info(f"No topic boundary detected: similarity={similarity}, new_terms={len(new_terms)}, chunk_length={len(self.stm_chunk)}")
        return False
    
    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """Jaccardé¡ä¼¼åº¦è¨ˆç®—ï¼ˆæ­£è¦åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é›†åˆï¼‰"""
        terms1 = set(self.extract_candidate_terms(text1))
        terms2 = set(self.extract_candidate_terms(text2))
        
        if not terms1 and not terms2:
            return 1.0
        if not terms1 or not terms2:
            return 0.0
        
        intersection = len(terms1 & terms2)
        union = len(terms1 | terms2)
        
        return intersection / union if union > 0 else 0.0
    
    def process_conversation_turn(self, text: str) -> Dict[str, any]:
        """
        ä¼šè©±ã‚¿ãƒ¼ãƒ³å‡¦ç†ï¼ˆASRâ†’ãƒ†ã‚­ã‚¹ãƒˆç¢ºå®šæ™‚ç‚¹ã§ãƒ•ãƒƒã‚¯ï¼‰
        æˆ»ã‚Šå€¤: {
            "is_boundary": bool,
            "glossary_updates": dict,
            "new_entry": str or None
        }
        """
        logger.info(f"ğŸ§  [SHORT_MEMORY] Processing conversation turn: '{text}'")
        
        # å€™è£œèªæŠ½å‡º
        candidate_terms = self.extract_candidate_terms(text)
        logger.info(f"ğŸ§  [SHORT_MEMORY] Extracted candidate terms: {candidate_terms}")
        
        # è¾æ›¸å‚ç…§
        found_meanings = self.check_glossary_reference(candidate_terms)
        if found_meanings:
            logger.info(f"ğŸ§  [SHORT_MEMORY] Found glossary meanings: {found_meanings}")
        
        # è¾æ›¸ç™»éŒ²ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        glossary_updates = self.apply_registration_heuristics(text, candidate_terms)
        
        # è©±é¡Œå¢ƒç•Œåˆ¤å®š
        is_boundary = self.detect_topic_boundary(text)
        logger.info(f"ğŸ§  [SHORT_MEMORY] Topic boundary result: {is_boundary}")
        
        # å¢ƒç•Œã§ãªã„å ´åˆï¼šstm_chunkã«è¿½åŠ 
        if not is_boundary:
            self.stm_chunk.append(text)
            logger.info(f"ğŸ§  [SHORT_MEMORY] Added to chunk. Current chunk length: {len(self.stm_chunk)}")
            return {
                "is_boundary": False,
                "glossary_updates": glossary_updates,
                "new_entry": None
            }
        
        # å¢ƒç•Œã®å ´åˆï¼šè¦ç´„ã—ã¦è¨˜éŒ²
        if self.stm_chunk:
            self.stm_chunk.append(text)
            summary = self.generate_one_sentence_diary(self.stm_chunk)
            logger.info(f"ğŸ§  [SHORT_MEMORY] Generated summary: '{summary}'")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            self.save_memory_entry(summary)
            
            # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
            self.stm_chunk = []
            self.stm_last_topic_repr = text
            
            return {
                "is_boundary": True,
                "glossary_updates": glossary_updates,
                "new_entry": summary
            }
        
        logger.info(f"ğŸ§  [SHORT_MEMORY] No chunk to process")
        return {
            "is_boundary": False,
            "glossary_updates": glossary_updates,
            "new_entry": None
        }
    
    def apply_registration_heuristics(self, text: str, terms: List[str]) -> Dict[str, str]:
        """è¾æ›¸ç™»éŒ²ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯"""
        updates = {}
        
        # å®šç¾©æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³
        definition_patterns = [
            r'(.+?)ã¨ã¯(.+?)ã®ã“ã¨ã§ã™',
            r'(.+?)ã¯(.+?)ã®ã“ã¨ã§ã™',
            r'(.+?)ã£ã¦(.+?)ã®ã“ã¨ã§ã™',
            r'(.+?)ã¨ã¯(.+?)ã§ã™'
        ]
        
        for pattern in definition_patterns:
            match = re.search(pattern, text)
            if match:
                term = match.group(1).strip()
                meaning = match.group(2).strip()
                if len(term) <= 10 and len(meaning) <= 50:
                    updates[term] = meaning
                    logger.info(f"ğŸ§  [SHORT_MEMORY] Detected definition: {term} = {meaning}")
        
        return updates
    
    def generate_one_sentence_diary(self, chunk: List[str]) -> str:
        """1æ–‡æ—¥è¨˜ã®ç”Ÿæˆï¼ˆå¢ƒç•Œæ™‚ã ã‘LLMå‘¼ã³å‡ºã—ï¼‰"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
        # å®Ÿéš›ã«ã¯OpenAI APIã‚’å‘¼ã³å‡ºã—ã¦è¦ç´„ã‚’ç”Ÿæˆ
        
        # ç°¡æ˜“å®Ÿè£…ï¼šä¼šè©±æŸã‚’çµåˆã—ã¦120å­—ä»¥å†…ã«è¦ç´„
        combined_text = " ".join(chunk)
        summary = combined_text[:120] + ("..." if len(combined_text) > 120 else "")
        
        # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼šæ”¹è¡Œ/å¤šçµµæ–‡å­—é™¤å»ã€å…¨è§’è¨˜å·çµ±ä¸€ã€æœ«å°¾ã€Œã€‚ã€ä»˜ä¸
        summary = re.sub(r'\n+', '', summary)
        summary = re.sub(r'[ã€‚ï¼ï¼Ÿ]+', 'ã€‚', summary)
        if not summary.endswith('ã€‚'):
            summary += 'ã€‚'
        
        return summary
    
    def save_memory_entry(self, sentence: str):
        """è¨˜æ†¶ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            # nekota-serverã®APIã‚’å‘¼ã³å‡ºã—
            api_url = "https://nekota-server-production.up.railway.app/api/memory/append"
            headers = {
                "Authorization": f"Bearer {self.get_jwt_token()}",
                "Content-Type": "application/json"
            }
            data = {"sentence": sentence}
            
            response = requests.post(api_url, json=data, headers=headers, timeout=10)
            
            if response.status_code == 200:
                logger.info(f"Saved memory entry for user {self.user_id}: {sentence}")
            else:
                logger.error(f"Failed to save memory entry: {response.status_code}")
            
        except Exception as e:
            logger.error(f"Error saving memory entry: {e}")
    
    def get_context_for_prompt(self) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        try:
            # nekota-serverã®APIã‚’å‘¼ã³å‡ºã—
            api_url = "https://nekota-server-production.up.railway.app/api/memory"
            headers = {"Authorization": f"Bearer {self.get_jwt_token()}"}
            
            response = requests.get(api_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if data and data.get("memory_text"):
                    memory_text = data["memory_text"]
                    # æœ€å¾Œã®300å­—ã‚’è¿”ã™
                    return memory_text[-300:] if len(memory_text) > 300 else memory_text
            
            return ""
            
        except Exception as e:
            logger.error(f"Error getting context for prompt: {e}")
            return ""
    
    def get_glossary_for_prompt(self, recent_terms: List[str]) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã®è¾æ›¸æƒ…å ±ã‚’å–å¾—ï¼ˆå¿…è¦èªã®ã¿ã€æœ€å¤§200å­—ï¼‰"""
        glossary_lines = []
        total_length = 0
        
        for term in recent_terms:
            if term in self.glossary_cache:
                meaning = self.glossary_cache[term]
                line = f"{term}: {meaning}"
                
                if total_length + len(line) > 200:
                    break
                
                glossary_lines.append(line)
                total_length += len(line) + 1  # +1 for newline
        
        return "\n".join(glossary_lines)
