# utils/short_memory_processor.py

import re
import json
import logging
from typing import List, Dict, Tuple, Optional
from datetime import datetime
try:
    import requests
except ImportError:
    # requestsãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯httpxã‚’ä½¿ç”¨
    import httpx as requests

logger = logging.getLogger(__name__)

class ShortMemoryProcessor:
    """çŸ­æœŸè¨˜æ†¶å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.glossary_cache = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã®è¾æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.stm_chunk = []  # åŒä¸€ãƒˆãƒ”ãƒƒã‚¯æŸã®ä¸€æ™‚è“„ç©
        self.stm_last_topic_repr = ""  # ç›´è¿‘ä»£è¡¨æ–‡
        self.jwt_token = None  # JWTãƒˆãƒ¼ã‚¯ãƒ³
        self.load_glossary_cache()
    
    def load_glossary_cache(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã«è¾æ›¸ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ­ãƒ¼ãƒ‰"""
        try:
            # nekota-serverã®APIã‚’å‘¼ã³å‡ºã—
            api_url = "https://nekota-server-production.up.railway.app/api/memory"
            headers = {"Authorization": f"Bearer {self.get_jwt_token()}"}
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data and data.get("glossary"):
                    self.glossary_cache = data["glossary"]
            else:
                self.glossary_cache = {}
            logger.info(f"Loaded glossary cache for user {self.user_id}: {len(self.glossary_cache)} terms")
        except Exception as e:
            logger.error(f"Error loading glossary cache: {e}")
            self.glossary_cache = {}
    
    def get_jwt_token(self):
        """JWTãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        if self.jwt_token:
            return self.jwt_token
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        logger.warning("ğŸ§  [SHORT_MEMORY] No JWT token available, using dummy")
        return "dummy_token"
    
    def extract_candidate_terms(self, text: str) -> List[str]:
        """å€™è£œèªã‚’æŠ½å‡ºï¼ˆå›ºæœ‰åè©ãƒ»ã‚«ã‚¿ã‚«ãƒŠèªãƒ»è¨˜å·æ··ã˜ã‚Šã®çèªãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼å£ç™–ï¼‰"""
        # æ­£è¦è¡¨ç¾ã§å€™è£œèªã‚’æŠ½å‡º
        pattern = r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥A-Za-z0-9]{2,})'
        matches = re.findall(pattern, text)
        
        # æ­£è¦åŒ–ã‚­ãƒ¼ã‚’ä½œæˆï¼ˆã²ã‚‰ãŒãªåŒ–/å°æ–‡å­—åŒ–/ç©ºç™½é™¤å»ï¼‰
        normalized_terms = []
        for match in matches:
            normalized = self.normalize_term(match)
            if normalized and len(normalized) >= 2:
                normalized_terms.append(normalized)
        
        return list(set(normalized_terms))  # é‡è¤‡é™¤å»
    
    def normalize_term(self, term: str) -> str:
        """èªå¥ã‚’æ­£è¦åŒ–"""
        # ã²ã‚‰ãŒãªåŒ–ã€å°æ–‡å­—åŒ–ã€ç©ºç™½é™¤å»
        term = term.lower().strip()
        # ç°¡å˜ãªæ­£è¦åŒ–ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã‚ˆã‚Šè©³ç´°ãªå‡¦ç†ãŒå¿…è¦ï¼‰
        return term
    
    def check_glossary_reference(self, terms: List[str]) -> Dict[str, str]:
        """è¾æ›¸å‚ç…§ï¼ˆãƒ¡ãƒ¢ãƒªå‚ç…§ã®ã¿ã€DB I/Oãªã—ï¼‰"""
        found_meanings = {}
        for term in terms:
            if term in self.glossary_cache:
                found_meanings[term] = self.glossary_cache[term]
        return found_meanings
    
    def detect_topic_boundary(self, current_text: str) -> bool:
        """è©±é¡Œå¢ƒç•Œåˆ¤å®šï¼ˆ3ç™ºè©±ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ–¹å¼ï¼‰"""
        # 3ç™ºè©±ã«é”ã—ãŸã‚‰å¿…ãšå¢ƒç•Œã¨ã™ã‚‹
        if len(self.stm_chunk) >= 2:  # ç¾åœ¨ã®ç™ºè©±ã‚’å«ã‚ã¦3ç™ºè©±ã«ãªã‚‹
            logger.info(f"ğŸ§  [SHORT_MEMORY] Topic boundary detected: 3 utterances reached (chunk_length={len(self.stm_chunk)})")
            return True
        
        logger.info(f"ğŸ§  [SHORT_MEMORY] No topic boundary: chunk_length={len(self.stm_chunk)}")
        return False
    
    
    def process_conversation_turn(self, text: str) -> Dict[str, any]:
        """
        ä¼šè©±ã‚¿ãƒ¼ãƒ³å‡¦ç†ï¼ˆASRâ†’ãƒ†ã‚­ã‚¹ãƒˆç¢ºå®šæ™‚ç‚¹ã§ãƒ•ãƒƒã‚¯ï¼‰
        æˆ»ã‚Šå€¤: {
            "is_boundary": bool,
            "glossary_updates": dict,
            "new_entry": str or None
        }
        """
        logger.info(f"ğŸ§  [SHORT_MEMORY] Processing conversation turn: '{text}'")
        
        # å€™è£œèªæŠ½å‡º
        candidate_terms = self.extract_candidate_terms(text)
        logger.info(f"ğŸ§  [SHORT_MEMORY] Extracted candidate terms: {candidate_terms}")
        
        # è¾æ›¸å‚ç…§
        found_meanings = self.check_glossary_reference(candidate_terms)
        if found_meanings:
            logger.info(f"ğŸ§  [SHORT_MEMORY] Found glossary meanings: {found_meanings}")
        
        # è¾æ›¸ç™»éŒ²ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        glossary_updates = self.apply_registration_heuristics(text, candidate_terms)
        
        # è©±é¡Œå¢ƒç•Œåˆ¤å®š
        is_boundary = self.detect_topic_boundary(text)
        logger.info(f"ğŸ§  [SHORT_MEMORY] Topic boundary result: {is_boundary}")
        
        # å¢ƒç•Œã§ãªã„å ´åˆï¼šstm_chunkã«è¿½åŠ 
        if not is_boundary:
            self.stm_chunk.append(text)
            logger.info(f"ğŸ§  [SHORT_MEMORY] Added to chunk. Current chunk length: {len(self.stm_chunk)}")
            return {
                "is_boundary": False,
                "glossary_updates": glossary_updates,
                "new_entry": None
            }
        
        # å¢ƒç•Œã®å ´åˆï¼šè¦ç´„ã—ã¦è¨˜éŒ²
        if self.stm_chunk:
            self.stm_chunk.append(text)
            summary = self.generate_one_sentence_diary(self.stm_chunk)
            logger.info(f"ğŸ§  [SHORT_MEMORY] Generated summary: '{summary}'")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            self.save_memory_entry(summary)
            
            # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
            self.stm_chunk = []
            self.stm_last_topic_repr = text
            
            return {
                "is_boundary": True,
                "glossary_updates": glossary_updates,
                "new_entry": summary
            }
        
        logger.info(f"ğŸ§  [SHORT_MEMORY] No chunk to process")
        return {
            "is_boundary": False,
            "glossary_updates": glossary_updates,
            "new_entry": None
        }
    
    def apply_registration_heuristics(self, text: str, terms: List[str]) -> Dict[str, str]:
        """è¾æ›¸ç™»éŒ²ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯"""
        updates = {}
        
        # å®šç¾©æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³
        definition_patterns = [
            r'(.+?)ã¨ã¯(.+?)ã®ã“ã¨ã§ã™',
            r'(.+?)ã¯(.+?)ã®ã“ã¨ã§ã™',
            r'(.+?)ã£ã¦(.+?)ã®ã“ã¨ã§ã™',
            r'(.+?)ã¨ã¯(.+?)ã§ã™'
        ]
        
        for pattern in definition_patterns:
            match = re.search(pattern, text)
            if match:
                term = match.group(1).strip()
                meaning = match.group(2).strip()
                if len(term) <= 10 and len(meaning) <= 50:
                    updates[term] = meaning
                    logger.info(f"ğŸ§  [SHORT_MEMORY] Detected definition: {term} = {meaning}")
        
        return updates
    
    def generate_one_sentence_diary(self, chunk: List[str]) -> str:
        """3ç™ºè©±ã®1æ–‡æ—¥è¨˜ç”Ÿæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªçµåˆæ–¹å¼ï¼‰"""
        if not chunk:
            return ""
        
        # 3ç™ºè©±ã‚’è‡ªç„¶ãª1æ–‡ã«çµ±åˆ
        if len(chunk) == 1:
            summary = chunk[0]
        elif len(chunk) == 2:
            # 2ç™ºè©±: "Aã€‚B" â†’ "Aã€‚B"
            summary = f"{chunk[0]}ã€‚{chunk[1]}"
        else:
            # 3ç™ºè©±: "Aã€‚Bã€‚C" â†’ "Aã€‚Bã€‚C"
            summary = f"{chunk[0]}ã€‚{chunk[1]}ã€‚{chunk[2]}"
        
        # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼šæ”¹è¡Œé™¤å»ã€å…¨è§’è¨˜å·çµ±ä¸€ã€æœ«å°¾ã€Œã€‚ã€ä»˜ä¸
        summary = re.sub(r'\n+', '', summary)
        summary = re.sub(r'[ã€‚ï¼ï¼Ÿ]+', 'ã€‚', summary)
        if not summary.endswith('ã€‚'):
            summary += 'ã€‚'
        
        # 120å­—ä»¥å†…ã«åˆ¶é™
        if len(summary) > 120:
            summary = summary[:117] + "..."
        
        logger.info(f"ğŸ§  [SHORT_MEMORY] Generated summary from {len(chunk)} utterances: '{summary}'")
        return summary
    
    def save_memory_entry(self, sentence: str):
        """è¨˜æ†¶ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            
            # nekota-serverã®APIã‚’å‘¼ã³å‡ºã—
            api_url = "https://nekota-server-production.up.railway.app/api/memory/append"
            headers = {
                "Authorization": f"Bearer {self.get_jwt_token()}",
                "Content-Type": "application/json"
            }
            data = {"sentence": sentence}
            
            logger.info(f"ğŸ§  [SHORT_MEMORY] Sending API request to: {api_url}")
            logger.info(f"ğŸ§  [SHORT_MEMORY] Request data: {data}")
            logger.info(f"ğŸ§  [SHORT_MEMORY] Request headers: {headers}")
            
            response = requests.post(api_url, json=data, headers=headers, timeout=10)
            
            logger.info(f"ğŸ§  [SHORT_MEMORY] API response status: {response.status_code}")
            logger.info(f"ğŸ§  [SHORT_MEMORY] API response content: {response.text}")
            logger.info(f"ğŸ§  [SHORT_MEMORY] API response headers: {dict(response.headers)}")
            
            if response.status_code == 200:
                logger.info(f"ğŸ§  [SHORT_MEMORY] Saved memory entry for user {self.user_id}: {sentence}")
            else:
                logger.error(f"ğŸ§  [SHORT_MEMORY] Failed to save memory entry: {response.status_code} - {response.text}")
            
        except Exception as e:
            logger.error(f"Error saving memory entry: {e}")
    
    def get_context_for_prompt(self) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        try:
            # nekota-serverã®APIã‚’å‘¼ã³å‡ºã—
            api_url = "https://nekota-server-production.up.railway.app/api/memory"
            headers = {"Authorization": f"Bearer {self.get_jwt_token()}"}
            
            response = requests.get(api_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if data and data.get("memory_text"):
                    memory_text = data["memory_text"]
                    # æœ€å¾Œã®300å­—ã‚’è¿”ã™
                    return memory_text[-300:] if len(memory_text) > 300 else memory_text
            
            return ""
            
        except Exception as e:
            logger.error(f"Error getting context for prompt: {e}")
            return ""
    
    def get_glossary_for_prompt(self, recent_terms: List[str]) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã®è¾æ›¸æƒ…å ±ã‚’å–å¾—ï¼ˆå¿…è¦èªã®ã¿ã€æœ€å¤§200å­—ï¼‰"""
        glossary_lines = []
        total_length = 0
        
        for term in recent_terms:
            if term in self.glossary_cache:
                meaning = self.glossary_cache[term]
                line = f"{term}: {meaning}"
                
                if total_length + len(line) > 200:
                    break
                
                glossary_lines.append(line)
                total_length += len(line) + 1  # +1 for newline
        
        return "\n".join(glossary_lines)
