import openai
from config import Config
from utils.logger import setup_logger
from utils.short_memory_processor import ShortMemoryProcessor

logger = setup_logger()

class LLMService:
    def __init__(self):
        self.client = openai.OpenAI(api_key=Config.OPENAI_API_KEY)
        self.model = Config.OPENAI_LLM_MODEL
        self.short_memory_processor = None  # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒè¨­å®šã•ã‚ŒãŸã‚‰åˆæœŸåŒ–
        logger.info(f"LLMService initialized with model: {self.model}")

    def set_user_id(self, user_id: str):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’è¨­å®šã—ã¦çŸ­æœŸè¨˜æ†¶ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’åˆæœŸåŒ–"""
        if not self.short_memory_processor:
            self.short_memory_processor = ShortMemoryProcessor(user_id)
            logger.info(f"Short memory processor initialized for user: {user_id}")
        else:
            logger.info(f"Short memory processor already exists for user: {user_id}")

    async def chat_completion(self, messages: list, stream: bool = False, user_id: str = None) -> str:
        try:
            # Add system prompt if not already present
            if not messages or messages[0]["role"] != "system":
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒæä¾›ã•ã‚ŒãŸå ´åˆã¯çŸ­æœŸè¨˜æ†¶ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’åˆæœŸåŒ–
                if user_id and not self.short_memory_processor:
                    self.set_user_id(user_id)
                
                # åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                system_prompt = Config.CHARACTER_PROMPT + """

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œè¦šãˆã¦ã€ã€Œè¦šãˆã¦ãŠã„ã¦ã€ã¨è¨€ã£ãŸæ™‚ã¯ã€ãã®æƒ…å ±ã‚’è¨˜æ†¶ã—ã¦ãã ã•ã„ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéå»ã®è©±é¡Œã«ã¤ã„ã¦è³ªå•ã—ãŸã‚‰ã€è¨˜æ†¶ã—ã¦ã„ã‚‹æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ç­”ãˆã¦ãã ã•ã„ã€‚"""
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                logger.info(f"ğŸ“ [PROMPT_DEBUG] Character prompt length: {len(Config.CHARACTER_PROMPT)} chars")
                logger.info(f"ğŸ“ [PROMPT_DEBUG] Character prompt preview: {Config.CHARACTER_PROMPT[:200]}...")
                
                # çŸ­æœŸè¨˜æ†¶ã¨è¾æ›¸ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
                if self.short_memory_processor:
                    try:
                        logger.info(f"ğŸ§  [PROMPT_INTEGRATION] Adding short memory and glossary context")
                        
                        # çŸ­æœŸè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                        memory_context = self.short_memory_processor.get_context_for_prompt()
                        if memory_context:
                            system_prompt += f"\n\n[çŸ­æœŸè¨˜æ†¶]\n{memory_context}"
                            logger.info(f"ğŸ§  [PROMPT_INTEGRATION] Added memory context: {memory_context[:100]}...")
                        else:
                            logger.info(f"ğŸ§  [PROMPT_INTEGRATION] No memory context available")
                        
                        # è¾æ›¸ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆæœ€è¿‘ã®ç”¨èªã‹ã‚‰ï¼‰
                        recent_terms = []
                        if len(messages) > 0:
                            # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ç”¨èªã‚’æŠ½å‡º
                            latest_message = messages[-1].get("content", "")
                            recent_terms = self.short_memory_processor.extract_candidate_terms(latest_message)
                            logger.info(f"ğŸ§  [PROMPT_INTEGRATION] Extracted recent terms: {recent_terms}")
                        
                        glossary_context = self.short_memory_processor.get_glossary_for_prompt(recent_terms)
                        if glossary_context:
                            system_prompt += f"\n\n[è¾æ›¸]\n{glossary_context}"
                            logger.info(f"ğŸ§  [PROMPT_INTEGRATION] Added glossary context: {glossary_context[:100]}...")
                        else:
                            logger.info(f"ğŸ§  [PROMPT_INTEGRATION] No glossary context available")
                    except Exception as e:
                        logger.error(f"ğŸ§  [PROMPT_INTEGRATION] Error adding context: {e}")
                        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã¯ç¶™ç¶š
                else:
                    logger.info(f"ğŸ§  [PROMPT_INTEGRATION] No short memory processor available")
                
                # æœ€çµ‚çš„ãªã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°å‡ºåŠ›
                logger.info(f"ğŸ“ [PROMPT_DEBUG] Final system prompt length: {len(system_prompt)} chars")
                logger.info(f"ğŸ“ [PROMPT_DEBUG] Final system prompt:\n{system_prompt}")
                
                messages.insert(0, {"role": "system", "content": system_prompt})
            
            # Use synchronous API call in async context
            import asyncio
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=500,
                    temperature=0.7,
                    stream=False  # Keep it simple for now
                )
            )
            
            result = response.choices[0].message.content if response.choices else ""
            logger.debug(f"LLM response: {result[:100]}...")
            return result
            
        except Exception as e:
            logger.error(f"LLM chat completion failed: {e}")
            return ""
